"""
Query API Endpoints
Main query execution and history with dataset integration
MODIFIED: History caching DISABLED - queries always fresh!
ADDED: Speech-to-Text transcription endpoint with OpenAI Whisper + ffmpeg conversion
"""

from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File
from pydantic import BaseModel
from sqlalchemy.orm import Session
import pandas as pd
import time
from datetime import datetime
from typing import Optional, List, Dict, Any
import tempfile
import os
import json
import anthropic

from app.db.session import get_db
from app.models.user import User
from app.models.query import QueryHistory
from app.models.dataset import Dataset
from app.schemas.query import (
    QueryExecuteRequest,
    QueryExecuteResponse,
    QueryHistoryResponse,
    QueryHistoryItem
)
from app.api.v1.auth import get_current_user
from app.core.claude_service import ClaudeService
from app.core.config import settings
from app.services.prompt_service import build_business_prompt

# ==========================================
# AI ANALYST CHAT - Schemas
# ==========================================

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatContext(BaseModel):
    query_text: str
    summary: str
    key_findings: List[str]
    recommendations: List[dict]
    risks: List[str]
    opportunities: List[str]

class ChatRequest(BaseModel):
    message: str
    context: ChatContext
    conversation_history: List[ChatMessage] = []

class ChatResponse(BaseModel):
    response: str
    success: bool = True

# ==========================================
# AI ANALYST ANALYZE - Request/Response Models
# ==========================================

class AnalyzeRequest(BaseModel):
    query: str
    code: str
    data_sample: List[dict]  # First 10 rows of actual table data
    total_rows: int
    columns: List[str]

class AnalysisResponse(BaseModel):
    analysis: str  # 2-3 sentence overview
    insights: List[str]  # 3-5 key findings
    recommendations: List[str]  # 2-3 recommendations

router = APIRouter(prefix="/query", tags=["Query"])


# ==========================================
# AI ANALYTIK - Business Insights Generator
# ==========================================
# UPDATED: Markdown output with dynamic topic context

async def generate_business_insights(
    query: str,
    result_df: pd.DataFrame,
    tenant_context: dict = None
) -> dict:
    """
    Generate business insights from query results using Claude
    
    Args:
        query: Original user query
        result_df: Pandas DataFrame with results
        tenant_context: Optional tenant-specific business context
    
    Returns:
        dict with raw_analysis markdown text and backward-compatible fields
    """
    
    if result_df is None or len(result_df) == 0:
        return {"success": False, "error": "No data to analyze"}
    
    # Prepare data for Claude - full table view
    result_str = result_df.to_string(index=False, max_rows=30)
    
    # Detect topic from query and columns for dynamic context
    query_lower = query.lower()
    
    # Determine analysis topic for dynamic context
    topic_context = ""
    if any(word in query_lower for word in ['plateb', 'payment', 'karta', 'card', 'paypal', 'bnpl', 'dob√≠rk']):
        topic_context = """
TR≈ΩN√ç KONTEXT PRO PLATEBN√ç METODY:
Pou≈æij sv√© znalosti o trendech v EU e-commerce platb√°ch:
- Pod√≠l karet vs. digit√°ln√≠ch penƒõ≈æenek vs. BNPL
- Trendy Apple Pay, Google Pay v CEE regionu
- Preference z√°kazn√≠k≈Ø podle segment≈Ø (B2B vs B2C)
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    elif any(word in query_lower for word in ['doprav', 'shipping', 'alzabox', 'bal√≠k', 'delivery', 'z√°silk']):
        topic_context = """
TR≈ΩN√ç KONTEXT PRO DOPRAVU:
Pou≈æij sv√© znalosti o last-mile delivery trendech:
- Click & Collect vs. home delivery trendy
- Same-day / next-day delivery v e-commerce
- V√Ωdejn√≠ boxy a jejich adopce v CEE
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    elif any(word in query_lower for word in ['segment', 'kategori', 'produkt', 'telefon', 'tv', 'poƒç√≠taƒç', 'spot≈ôebiƒç']):
        topic_context = """
TR≈ΩN√ç KONTEXT PRO PRODUKTOV√â SEGMENTY:
Pou≈æij sv√© znalosti o e-commerce kategori√≠ch:
- V√Ωvoj popt√°vky po elektronice v EU
- Mar≈æe v r≈Øzn√Ωch kategori√≠ch
- Sez√≥nnost a trendy
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    elif any(word in query_lower for word in ['z√°kazn', 'customer', 'b2b', 'b2c', 'alzaplus', 'vƒõrnost', 'loyalty']):
        topic_context = """
TR≈ΩN√ç KONTEXT PRO Z√ÅKAZN√çKY:
Pou≈æij sv√© znalosti o z√°kaznick√Ωch trendech:
- B2B vs B2C chov√°n√≠ v e-commerce
- Loyalty programy a jejich efektivita
- Customer retention benchmarky
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    elif any(word in query_lower for word in ['zem', 'country', 'czech', 'slovak', 'hungary', 'austria', 'nƒõmecko', 'rakousko']):
        topic_context = """
TR≈ΩN√ç KONTEXT PRO GEOGRAFII:
Pou≈æij sv√© znalosti o e-commerce v regionu:
- E-commerce penetrace v jednotliv√Ωch zem√≠ch CEE
- R≈Østov√© trendy podle trhu
- Specifika jednotliv√Ωch trh≈Ø
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    elif any(word in query_lower for word in ['n√°klad', 'cost', 'spot≈ôeb', 'materi√°l', 'energie', 'pl', 'p&l', 'v√Ωkaz']):
        topic_context = """
TR≈ΩN√ç KONTEXT PRO N√ÅKLADY A P&L:
Pou≈æij sv√© znalosti o n√°kladov√Ωch struktur√°ch:
- Typick√© n√°kladov√© pomƒõry v e-commerce/retail
- Energie a materi√°l jako % tr≈æeb
- Optimalizaƒçn√≠ p≈ô√≠le≈æitosti
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    elif any(word in query_lower for word in ['ko≈°√≠k', 'aov', 'order value', 'objedn√°v', 'transakc']):
        topic_context = """
TR≈ΩN√ç KONTEXT PRO KO≈†√çK/AOV:
Pou≈æij sv√© znalosti o e-commerce metrik√°ch:
- Pr≈Ømƒõrn√© hodnoty ko≈°√≠ku v CEE e-commerce
- Faktory ovliv≈àuj√≠c√≠ AOV
- Cross-sell a up-sell strategie
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    else:
        topic_context = """
TR≈ΩN√ç KONTEXT:
Pokud m√°≈° relevantn√≠ znalosti o tomto t√©matu z e-commerce nebo retail prost≈ôed√≠, pou≈æij je.
D≈ÆLE≈ΩIT√â: Uveƒè pouze informace, kter√© skuteƒçnƒõ zn√°≈°. Nevym√Ω≈°lej konkr√©tn√≠ ƒç√≠sla."""

    # Build AI Analytik prompt - MARKDOWN output
    ai_prompt = f"""Jsi senior finanƒçn√≠ analytik Alza.cz (5+ let ve firmƒõ) p≈ôipravuj√≠c√≠ koment√°≈ô k dat≈Øm pro CFO.

BUSINESS KONTEXT ALZA:
- Nejvƒõt≈°√≠ e-commerce v ƒåR, p≈Øsob√≠ v CZ, SK, HU, AT, DE
- Hlavn√≠ segmenty: Telefony, TV/Audio, Poƒç√≠taƒçe, Spot≈ôebiƒçe, Gaming
- AlzaPlus+ = vƒõrnostn√≠ program (ni≈æ≈°√≠ ko≈°√≠k, vy≈°≈°√≠ frekvence, lep≈°√≠ retence)
- B2B = firemn√≠ z√°kazn√≠ci (vƒõt≈°√≠ objedn√°vky, ni≈æ≈°√≠ mar≈æe)
- Sez√≥nnost: Q4 (Black Friday, V√°noce) = peak, Q1 = √∫tlum

DOTAZ U≈ΩIVATELE:
{query}

DATA:
{result_str}

{topic_context}

STRUKTURA ODPOVƒöDI (pi≈° plynul√Ω text v markdown form√°tu):

## üìà Dynamika dat

Popi≈° konkr√©tn√≠ trend z dat:
- R≈Øst/pokles z X na Y (absolutn√≠ zmƒõna)
- Procentu√°ln√≠ zmƒõna: +/- X%
- Pro v√≠ce obdob√≠: YoY, MoM zmƒõny
- Pro statick√° data: rozlo≈æen√≠ a koncentrace (top 3 tvo≈ô√≠ X%)

## üíº Business zhodnocen√≠

Je tento v√Ωvoj POZITIVN√ç nebo NEGATIVN√ç pro Alzu? Proƒç?
- Implikace pro tr≈æby, mar≈æe, n√°klady
- Dopad na budouc√≠ r≈Øst a profitabilitu
- Kontext v r√°mci Alza strategie

## ‚ö†Ô∏è Rizika

Identifikuj 2-3 hlavn√≠ rizika:
- **[N√°zev rizika]**: Popis co hroz√≠ a jak se tomu vyhnout

## üöÄ P≈ô√≠le≈æitosti a doporuƒçen√≠

- Konkr√©tn√≠ p≈ô√≠le≈æitosti k r≈Østu
- Actionable doporuƒçen√≠ (co udƒõlat)
- Tr≈æn√≠ kontext pokud je relevantn√≠

PRAVIDLA:
- Data z tabulky = fakta, MUS√ç b√Ωt 100% p≈ôesn√°
- Tr≈æn√≠ kontext = tv√© znalosti, pouze pokud jsi si jist√Ω
- Form√°t ƒç√≠sel: 1 234 567 Kƒç, procenta s 1 desetinn√Ωm (15.3%)
- Pi≈° ƒçesky, profesion√°lnƒõ, konkr√©tnƒõ
- NIKDY si nevym√Ω≈°lej statistiky nebo ƒç√≠sla
- Pokud tr≈æn√≠ kontext nezn√°≈°, vynech ho

Zaƒçni p≈ô√≠mo sekc√≠ "## üìà Dynamika dat":"""

    # Call Claude API
    try:
        client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        
        message = client.messages.create(
            model=settings.ANTHROPIC_MODEL,
            max_tokens=2000,
            messages=[{
                "role": "user",
                "content": ai_prompt
            }]
        )
        
        # Get raw markdown response
        raw_analysis = message.content[0].text
        
        print(f"‚úÖ AI Insights generated successfully (markdown format)")
        
        # Return new format with raw_analysis + backward-compatible fields
        return {
            "success": True,
            "insights": {
                "raw_analysis": raw_analysis,
                # Backward compatibility - extract summary from first paragraph
                "summary": _extract_summary(raw_analysis),
                "key_findings": [],
                "recommendations": [],
                "risks": [],
                "opportunities": [],
                "next_steps": [],
                "context_notes": None
            }
        }
        
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate AI insights: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


def _extract_summary(markdown_text: str) -> str:
    """Extract first meaningful paragraph as summary for backward compatibility"""
    lines = markdown_text.split('\n')
    for line in lines:
        line = line.strip()
        # Skip headers and empty lines
        if line and not line.startswith('#') and not line.startswith('-') and not line.startswith('*') and len(line) > 50:
            return line[:300] + '...' if len(line) > 300 else line
    return "Anal√Ωza dat provedena."


# ==========================================
# EXECUTE QUERY
# ==========================================

@router.post("/execute", response_model=QueryExecuteResponse)
async def execute_query(
    query_request: QueryExecuteRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Execute natural language query with datasets
    
    Process:
    1. Load tenant's datasets into DataFrames
    2. Generate Python code via Claude with Alza business prompts
    3. Execute code safely with datasets
    4. Return results
    5. NO CACHING - always fresh results!
    
    Requires: Bearer token
    """
    
    start_time = time.time()
    
    try:
        # Initialize Claude service
        claude_service = ClaudeService(api_key=settings.ANTHROPIC_API_KEY)
        
        # Load datasets for this tenant
        datasets_query = db.query(Dataset).filter(
            Dataset.tenant_id == current_user.tenant_id
        )
        
        # Filter by specific datasets if requested
        if query_request.dataset_ids:
            datasets_query = datasets_query.filter(
                Dataset.id.in_(query_request.dataset_ids)
            )
        
        datasets = datasets_query.all()
        
        # Load DataFrames
        dataframes = {}
        dataset_info = []
        available_dataset_names = []
        
        for dataset in datasets:
            try:
                # ==========================================
                # üîß FIX: Proper CSV loading with encoding
                # ==========================================
                if dataset.filename.endswith('.csv'):
                    # Try multiple encodings and separators
                    try:
                        # Czech format: UTF-8, semicolon, comma decimal
                        df = pd.read_csv(
                            dataset.file_path,
                            encoding='utf-8',
                            sep=';',
                            decimal=',',
                            low_memory=False
                        )
                    except Exception as e1:
                        try:
                            # Standard format: UTF-8, comma, dot decimal
                            df = pd.read_csv(
                                dataset.file_path,
                                encoding='utf-8',
                                sep=',',
                                decimal='.',
                                low_memory=False
                            )
                        except Exception as e2:
                            try:
                                # Windows format: Windows-1250, semicolon
                                df = pd.read_csv(
                                    dataset.file_path,
                                    encoding='windows-1250',
                                    sep=';',
                                    decimal=',',
                                    low_memory=False
                                )
                            except Exception as e3:
                                print(f"Warning: Could not load dataset {dataset.original_filename}: {e1}")
                                continue
                else:
                    # Excel files
                    df = pd.read_excel(dataset.file_path)
                
                # Use original filename without extension as variable name
                var_name = dataset.original_filename.rsplit('.', 1)[0].replace(' ', '_').replace('-', '_')
                dataframes[var_name] = df
                available_dataset_names.append(dataset.original_filename)
                
                dataset_info.append({
                    "name": var_name,
                    "original_filename": dataset.original_filename,
                    "rows": len(df),
                    "columns": list(df.columns)
                })
                
                # Update last_used_at
                dataset.last_used_at = datetime.utcnow()
                
            except Exception as e:
                print(f"Warning: Could not load dataset {dataset.original_filename}: {e}")
        
        db.commit()
        
        # ==========================================
        # üóìÔ∏è PERIOD VALIDATION FOR WIDE FORMAT
        # ==========================================
        # Check if datasets use WIDE format (date columns like "01.01.2024")
        has_wide_format = False
        print(f"üîç Checking for WIDE format in {len(dataframes)} dataframes...")
        
        for df_name, df in dataframes.items():
            # Check if columns contain date patterns
            date_columns = [col for col in df.columns if isinstance(col, str) and 
                          (col.startswith('01.') or col.startswith('02.') or 
                           col.startswith('03.') or col.startswith('04.') or 
                           col.startswith('05.') or col.startswith('06.') or 
                           col.startswith('07.') or col.startswith('08.') or 
                           col.startswith('09.') or col.startswith('10.') or 
                           col.startswith('11.') or col.startswith('12.'))]
            if date_columns:
                print(f"‚úÖ WIDE format detected in {df_name}: {len(date_columns)} date columns")
                has_wide_format = True
                break
        
        print(f"üóìÔ∏è Has WIDE format: {has_wide_format}")
        
        # If WIDE format, check if user specified period
        if has_wide_format:
            query_lower = query_request.query.lower()
            print(f"üîç Query (lowercase): '{query_lower}'")
            
            # Period keywords (Czech months, years, quarters)
            period_keywords = [
                'leden', '√∫nor', 'b≈ôezen', 'duben', 'kvƒõten', 'ƒçerven',
                'ƒçervenec', 'srpen', 'z√°≈ô√≠', '≈ô√≠jen', 'listopad', 'prosinec',
                'january', 'february', 'march', 'april', 'may', 'june',
                'july', 'august', 'september', 'october', 'november', 'december',
                'q1', 'q2', 'q3', 'q4', 'kvart√°l', 'pololet√≠', 'rok',
                '202', '2025', '2024', '2023',  # Years
                '01.', '02.', '03.', '04.', '05.', '06.',  # Date formats
                '07.', '08.', '09.', '10.', '11.', '12.'
            ]
            
            has_period = any(keyword in query_lower for keyword in period_keywords)
            print(f"üóìÔ∏è Has period in query: {has_period}")
            
            if not has_period:
                print("üö´ PERIOD VALIDATION FAILED - Returning error")
                # Return error requiring period specification
                return QueryExecuteResponse(
                    success=False,
                    hasMetadata=False,
                    periodValidationFailed=True,
                    query_text=query_request.query,
                    generated_code="",
                    result=[],
                    result_rows=0,
                    execution_time_ms=0,
                    query_id="",
                    datasets_used=[],
                    error_message="Pro anal√Ωzu ƒçasov√Ωch dat pros√≠m specifikujte obdob√≠ (nap≈ô. 'leden 2024', 'Q1 2025', '01.01.2024')"
                )
            else:
                print("‚úÖ Period validation passed - continuing...")
        
        # ==========================================
        # ‚ö° Use Alza business prompt builder
        # ==========================================
        
        prompt = build_business_prompt(
            user_query=query_request.query,
            available_datasets=available_dataset_names
        )
        
        # ==========================================
        # üîó ADD CONTEXT FROM PREVIOUS QUERY
        # ==========================================
        if query_request.context:
            # Extract query chain if available
            query_chain = query_request.context.get('query_chain', [])
            chain_length = len(query_chain) + 1  # +1 for current query
            
            # üÜï OPTIMIZE: Use simplified prompt for deep drill-down (3+ levels)
            if chain_length >= 3:
                # SIMPLIFIED PROMPT FOR DEEP DRILL-DOWN
                context_section = f"""

## üîó MULTI-LEVEL DRILL-DOWN CONTEXT

**üö® CRITICAL FOR LEVEL 3+ QUERIES:**

When doing 3rd or more follow-up query:
1. **Review ENTIRE query chain**, not just immediate previous!
2. **Extract original filters from Query 1** (usually has main context)
3. **Preserve these filters through all levels**

**Example chain:**
```
Query 1: "Spot≈ôeba materi√°lu a energie leden 2024" (PL.csv)
  ‚Üí Filters: Analytical account in [501200, 502100, ...], jan_col = '01.01.2024'
  
Query 2: "Top dodavatel√©" (OVH.csv)
  ‚Üí Applied Query 1 filters ‚úÖ
  ‚Üí Result: Top 10 suppliers for materi√°l+energie
  
Query 3: "Jednotliv√° ELD" (OVH.csv)
  ‚Üí MUST apply Query 1 filters (Analytical account) ‚úÖ
  ‚Üí OPTIONALLY filter by Query 2 results (top suppliers)
  ‚Üí ‚ùå WRONG: Only filtering by time (gets ALL ELD in Jan)
```

**Code template for Level 3:**
```python
# Load OVH
ovh = OVH.copy()

# Filter 1: TIME from Query 1
jan_col = '01.01.2024'

# Filter 2: ANALYTICAL ACCOUNT from Query 1  
# Extract from Query 1 context (look for account numbers or Acc-Level categories)
account_numbers = [501200, 502100, 502200, ...]  # From "materi√°l a energie"
ovh_filtered = ovh[ovh['Analytical account'].isin(account_numbers)]

# Filter 3 (optional): SUPPLIERS from Query 2
# If Query 2 showed "top dodavatel√©", could filter by those
top_suppliers = ['ENIC s.r.o.', 'Pra≈æsk√° energetika', ...]
ovh_filtered = ovh_filtered[ovh_filtered['Customer/company name'].isin(top_suppliers)]

# Now get ELD details
eld_details = ovh_filtered[ovh_filtered[jan_col] != 0]
```

**Key principle:** 
- Query 1 establishes SCOPE (time + category)
- Query 2+ drills down WITHIN that scope
- Never lose the original scope!

 (Level {chain_length})

**Query Chain:**
{chr(10).join([f"{i+1}. {q}" for i, q in enumerate(query_chain)])}
‚Üí **CURRENT:** {query_request.query}

**Previous Code (EXTRACT FILTERS FROM THIS):**
```python
{query_request.context.get('code', 'N/A')}
```

**üîç DATASET DETECTION:**
Previous code used: {"PL.csv" if "PL.copy()" in query_request.context.get('code', '') else "OVH.csv" if "OVH.copy()" in query_request.context.get('code', '') else "Sales.csv" if "Sales.copy()" in query_request.context.get('code', '') else "M3.csv" if "M3.copy()" in query_request.context.get('code', '') else "Unknown"}

**‚ö†Ô∏è CONTINUE WITH THE SAME DATASET!**

**Previous Result:** {query_request.context.get('result_summary', 'N/A').split('First result row:')[0]}

---

## ‚ö†Ô∏è CRITICAL RULES FOR LEVEL {chain_length}:

**1. EXTRACT & REUSE ALL FILTERS:**
- Find time column from previous code (e.g., `col = '01.01.2024'`)
- Find which datasets were used (M3, Sales, Documents, etc.)
- Identify ALL dimension filters already applied

**2. CREATE CROSS-DIMENSIONAL BREAKDOWN:**
- Level {chain_length} MUST combine ALL previous dimensions + new dimension
- Example for 3 levels (Time ‚Üí Customer Type ‚Üí AlzaPlus):
  * AlzaPlus + B2B
  * AlzaPlus + B2C
  * Non-AlzaPlus + B2B
  * Non-AlzaPlus + B2C

**3. APPLY FILTERS TO ALL DATASETS:**
If previous code used multiple datasets (e.g., M3 + Sales for margin):
```python
# ‚úÖ CORRECT - Apply filters to BOTH datasets:
m3_filtered = m3[(m3['AlzaPlus+'] == 'AlzaPlus+') & (m3['Customer is business'] == 'Yes')]
sales_filtered = sales[(sales['AlzaPlus+'] == 'AlzaPlus+') & (sales['Customer is business'] == 'Yes')]
```

**4. CODE TEMPLATE:**
```python
import pandas as pd

# Load datasets (same as previous)
m3 = M3.copy()
sales = Sales.copy()

# Extract time column from previous code
col = '01.01.2024'  # ‚Üê COPY FROM PREVIOUS CODE!

# Create combinations of ALL dimensions
results = []

for dimension1_val in ['Value1', 'Value2']:  # Previous dimension
    for dimension2_val in ['ValueA', 'ValueB']:  # New dimension
        # Filter BOTH datasets with ALL filters
        m3_subset = m3[(m3['Dim1'] == dimension1_val) & (m3['Dim2'] == dimension2_val)]
        sales_subset = sales[(sales['Dim1'] == dimension1_val) & (sales['Dim2'] == dimension2_val)]
        
        # Calculate metric (same formula as previous)
        m3_value = m3_subset[col].sum()
        sales_value = sales_subset[col].sum()
        margin_pct = (m3_value / sales_value * 100) if sales_value > 0 else 0
        
        results.append({{
            'Segment': f'{{dimension2_val}} + {{dimension1_val}}',
            'M3 mar≈æe (Kƒç)': m3_value,
            'Tr≈æby (Kƒç)': sales_value,
            'M3 mar≈æe %': margin_pct
        }})

result = pd.DataFrame(results)
```

**REMEMBER:** Use EXACT same time column and datasets as previous code!
"""
            else:
                # STANDARD PROMPT FOR LEVELS 1-2
                query_chain_text = ""
                if query_chain and len(query_chain) > 0:
                    query_chain_text = f"""
**üîó QUERY CHAIN:**
{chr(10).join([f"{i+1}. {q}" for i, q in enumerate(query_chain)])}
‚Üí Current: {query_request.query}
"""
                
                context_section = f"""

## ‚ö†Ô∏è CONTEXT FROM PREVIOUS QUERY

{query_chain_text}

**Previous Question:** {query_request.context.get('query', 'N/A')}

**Previous Code:**
```python
{query_request.context.get('code', 'N/A')}
```

**üîç DATASET DETECTED IN PREVIOUS QUERY:**
‚Üí **Previous used: {'PL.csv (P&L)' if 'PL.copy()' in query_request.context.get('code', '') or 'pl = PL' in query_request.context.get('code', '').lower() else 'OVH.csv (detailed expenses)' if 'OVH.copy()' in query_request.context.get('code', '') or 'ovh = OVH' in query_request.context.get('code', '').lower() else 'M3.csv (margins)' if 'M3.copy()' in query_request.context.get('code', '') else 'Sales.csv (revenue)' if 'Sales.copy()' in query_request.context.get('code', '') else 'Documents.csv (orders)' if 'Documents.copy()' in query_request.context.get('code', '') else 'Unknown'}**

**‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL: CONTINUE USING THE SAME DATASET!**
- If previous used PL.csv ‚Üí CONTINUE with PL.csv!
- If previous used OVH.csv ‚Üí CONTINUE with OVH.csv!
- If previous used Sales.csv ‚Üí CONTINUE with Sales.csv!
- DO NOT switch datasets unless user explicitly asks!

**Previous Result Summary:**
{query_request.context.get('result_summary', 'N/A')}

**üî¥ FOLLOW-UP RULES:**

1. **MAINTAIN SCOPE:** Use the SAME time period from previous query!

2. **EXTRACT FILTERS:** From previous code, identify:
   - Date columns (e.g., `col = '01.05.2025'`)
   - Filters applied (segment, country, customer type)
   - Which datasets were used (one or multiple)


3. **üö® CRITICAL - CROSS-DATASET FOLLOW-UP (PL ‚Üí OVH):**

When previous query used **PL.csv** and current asks about **dodavatel√©/suppliers/vendors**:

**YOU MUST:**
- Switch to **OVH.csv** (has supplier details in "Customer/company name")
- **APPLY TIME FILTER** from previous query
- **USE Acc-Level 1 or 2** for category filtering (NOT Acc-Level 3!)

**‚ö†Ô∏è CRITICAL - How to link PL.csv and OVH.csv:**

Use **"Analytical account"** field - it's the SAME in both datasets!
- "Analytical account" = Account number (501200, 502100, etc.)
- ‚úÖ EXACT match between PL and OVH
- ‚úÖ Most precise way to filter

**Alternative (if needed):**
- Acc-Level 1: SAME in both ‚úÖ ("Re≈æijn√≠ n√°klady")
- Acc-Level 2: SAME in both ‚úÖ ("Spot≈ôeba materi√°lu a slu≈æeb")
- Acc-Level 3: DIFFERENT ‚ùå (PL has "Materi√°l", OVH has "Office supplies")

**üî• BEST PRACTICE - Use Analytical account:**

**Example:**
```python
Previous PL query: "Spot≈ôeba materi√°lu a energie v lednu 2024"
  - Filtered: pl[pl['Acc-Level 3'].isin(['Materi√°l', 'Energie'])]
  - Got accounts: 501200, 502100, 502200, etc.
  
Current OVH query: "Top dodavatel√©"

# ‚úÖ BEST - Use Analytical account (most precise):
ovh = OVH.copy()
jan_col = '01.01.2024'

# Extract account numbers from previous PL filter
pl_previous = pl[pl['Acc-Level 3'].isin(['Materi√°l', 'Energie'])]
account_numbers = pl_previous['Analytical account'].unique()

# Apply to OVH
ovh_filtered = ovh[ovh['Analytical account'].isin(account_numbers)]
suppliers = ovh_filtered.groupby('Customer/company name')[jan_col].sum()
top_suppliers = suppliers.nlargest(10)

# ‚úÖ ALTERNATIVE - Use Acc-Level 2 (broader):
ovh_filtered = ovh[ovh['Acc-Level 2'] == 'Spot≈ôeba materi√°lu a slu≈æeb']

# ‚ùå WRONG - Using Acc-Level 3:
ovh_filtered = ovh[ovh['Acc-Level 3'].isin(['Materi√°l', 'Energie'])]  # Empty!
```

**Summary:**
1. BEST: Use "Analytical account" for precise filtering
2. GOOD: Use "Acc-Level 1" or "Acc-Level 2" for broader filtering
3. NEVER: Use "Acc-Level 3" across datasets (different values!)

**Why OVH.csv?**
- PL.csv = Aggregated P&L (no supplier names)
- OVH.csv = Detailed expense documents with suppliers
- To see WHO we paid, use OVH.csv!


4. **FOR MULTI-DATASET QUERIES (AOV, M3 mar≈æe):**
   
   If previous used TWO datasets (e.g., Sales + M3):
   - Apply filters to BOTH datasets
   - Use SAME time column on both
   
   Example:
   ```python
   m3 = M3.copy()
   sales = Sales.copy()
   col = '01.05.2025'  # ‚Üê SAME as previous
   
   # B2B - filter BOTH datasets:
   b2b_m3 = m3[m3['Customer is business customer (IN/TIN)'] == 'Customer is business customer (IN/TIN)']
   b2b_sales = sales[sales['Customer is business customer (IN/TIN)'] == 'Customer is business customer (IN/TIN)']
   b2b_margin = b2b_m3[col].sum() / b2b_sales[col].sum() * 100
   ```

5. **REUSE FILTERS:** Apply same filters in new code!

6. **BUILD UPON RESULTS:** Drill down the SAME data, not different period!
"""
            prompt += context_section
        
        # ==========================================
        # üÜï ADD TIME-SERIES EXAMPLE FOR WIDE FORMAT
        # ==========================================
        time_series_example = """

## IMPORTANT: MONTHLY TREND ANALYSIS IN WIDE FORMAT

If user asks for monthly trends (e.g., "v√Ωvoj tr≈æeb po mƒõs√≠c√≠ch", "monthly revenue trend"), use this pattern:

```python
import pandas as pd

# Copy DataFrame
sales = Sales.copy()

# Find all 2024 date columns (format: DD.MM.YYYY)
date_cols_2024 = [col for col in sales.columns 
                  if '2024' in col and '.' in col]

# Sort chronologically
date_cols_2024 = sorted(date_cols_2024, 
                       key=lambda x: pd.to_datetime(x, format='%d.%m.%Y'))

# Calculate monthly revenue
monthly_data = []
for month_col in date_cols_2024:
    revenue = sales[month_col].sum()
    monthly_data.append({
        'Mƒõs√≠c': month_col,
        'Tr≈æby': revenue
    })

result = pd.DataFrame(monthly_data)

# Add MoM% change
result['MoM %'] = result['Tr≈æby'].pct_change() * 100

# Format
result['Tr≈æby (Kƒç)'] = result['Tr≈æby'].apply(lambda x: f'{x:,.0f}'.replace(',', ' '))
result['MoM %'] = result['MoM %'].apply(lambda x: f'{x:+.1f}%' if pd.notna(x) else '-')

result = result[['Mƒõs√≠c', 'Tr≈æby (Kƒç)', 'MoM %']]
```

CRITICAL: Use this exact pattern for time-series queries. Do NOT use melt/unpivot, do NOT look for 'order_date' column!
"""
        
        prompt += time_series_example
        
        print(f"\n{'='*60}")
        print(f"üìä Query: {query_request.query}")
        print(f"üìÅ Available datasets: {', '.join(available_dataset_names)}")
        print(f"{'='*60}\n")
        
        # Generate code via Claude
        print(f"Generating code with Alza business prompts...")
        generated_code = claude_service.generate_python_code(prompt, max_tokens=2000)
        
        # Clean up code (remove markdown if present)
        clean_code = claude_service.extract_python_code(generated_code)
        if not clean_code:
            clean_code = generated_code.strip()
        
        # ==========================================
        # üîß FIX: Remove file reading from generated code
        # ==========================================
        # Replace pd.read_csv('filename') with DataFrame variable
        for var_name, original_name in [(v, d["original_filename"]) for v, d in zip(dataframes.keys(), dataset_info)]:
            # Replace all variants of reading the file
            clean_code = clean_code.replace(
                f"pd.read_csv('{original_name}'",
                f"{var_name}.copy()  # Already loaded"
            )
            clean_code = clean_code.replace(
                f'pd.read_csv("{original_name}"',
                f'{var_name}.copy()  # Already loaded'
            )
            # Also handle uppercase DataFrame names
            upper_var = var_name.upper() if var_name.islower() else var_name
            clean_code = clean_code.replace(
                f"{upper_var} = pd.read_csv",
                f"# {upper_var} already loaded\n# "
            )
        
        print(f"Generated code:\n{clean_code}\n")
        
        # ==========================================
        # üìä EXECUTE CODE WITH ENHANCED ERROR LOGGING
        # ==========================================
        error_message = None
        success = True
        result_rows = None
        
        try:
            # Create safe execution environment with datasets
            safe_globals = {
                "pd": pd,
                "datetime": datetime,
                **dataframes  # Add all loaded DataFrames
            }
            safe_locals = {}
            
            # Execute generated code
            exec(clean_code, safe_globals, safe_locals)
            
            # Get result
            if 'result' in safe_locals:
                result_value = safe_locals['result']
                
                # Handle list containing single DataFrame (Claude sometimes does this)
                if isinstance(result_value, list) and len(result_value) == 1 and isinstance(result_value[0], pd.DataFrame):
                    result_value = result_value[0]  # Extract DataFrame from list
            else:
                raise ValueError("No 'result' variable in generated code")
            
            # Convert result to JSON
            if isinstance(result_value, pd.DataFrame):
                result_json = result_value.to_dict(orient='records')
                result_rows = len(result_value)
            elif isinstance(result_value, pd.Series):
                result_json = result_value.to_dict()
                result_rows = len(result_value)
            elif isinstance(result_value, (list, dict)):
                result_json = result_value
                result_rows = len(result_value) if isinstance(result_value, list) else 1
            else:
                result_json = {"value": str(result_value)}
                result_rows = 1
                
        except Exception as e:
            success = False
            error_message = str(e)
            result_json = None
            result_rows = None
            
            # ‚ö†Ô∏è ENHANCED ERROR LOGGING - Print code and error details
            print(f"‚ùå Execution error: {error_message}")
            print(f"\n{'='*60}")
            print(f"‚ö†Ô∏è  FAILED CODE:")
            print(f"{'='*60}")
            print(clean_code)
            print(f"{'='*60}")
            print(f"‚ö†Ô∏è  ERROR DETAILS:")
            print(f"{'='*60}")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {error_message}")
            
            # Try to get line number if possible
            import traceback
            print(f"\nFull traceback:")
            traceback.print_exc()
            print(f"{'='*60}\n")
        
        # Calculate execution time
        execution_time_ms = int((time.time() - start_time) * 1000)
        
        # ==========================================
        # ‚úÖ SAVE TO HISTORY (only successful queries with results)
        # ==========================================
        query_id = "no-cache"  # Default
        
        if success and result_json:  # Only save successful queries with actual results
            try:
                # Create history record
                history_record = QueryHistory(
                    tenant_id=current_user.tenant_id,  # ‚úÖ User's tenant
                    user_id=current_user.id,
                    query_text=query_request.query,
                    generated_code=clean_code,
                    result=result_json,
                    result_rows=result_rows,
                    execution_time_ms=execution_time_ms,
                    success=True,
                    error_message=None,
                    datasets_used=[str(d.id) for d in datasets] if datasets else None
                )
                db.add(history_record)
                db.commit()
                db.refresh(history_record)
                
                query_id = str(history_record.id)
                print(f"‚úÖ Query saved to history: {query_id}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to save history (non-critical): {e}")
                db.rollback()
        else:
            print(f"‚ö†Ô∏è Query not saved (failed or no results)")
        
        print(f"‚úÖ Query executed in {execution_time_ms}ms\n")
        
        # ==========================================
        # üÜï GENERATE AI INSIGHTS (ON-DEMAND ONLY)
        # ==========================================
        ai_insights = None
        if success and result_json and query_request.generate_insights:  # ‚Üê Only when user requests!
            # Get DataFrame for analysis (before JSON conversion)
            try:
                # result_value is the DataFrame we extracted from exec
                insights_df = result_value if isinstance(result_value, pd.DataFrame) else None
                
                if insights_df is not None:
                    print(f"ü§ñ Generating AI business insights (on-demand)...")
                    insights_result = await generate_business_insights(
                        query=query_request.query,
                        result_df=insights_df,
                        tenant_context=None  # Can add per-tenant context later
                    )
                    if insights_result["success"]:
                        ai_insights = insights_result["insights"]
                        print(f"‚úÖ AI Insights ready")
                    else:
                        print(f"‚ö†Ô∏è AI Insights failed: {insights_result.get('error')}")
            except Exception as e:
                print(f"‚ö†Ô∏è AI Insights generation error: {e}")
        elif success and result_json and not query_request.generate_insights:
            print(f"‚ÑπÔ∏è AI Insights skipped (not requested)")
        
        # Return response
        return QueryExecuteResponse(
            query_id=query_id,
            success=success,
            query_text=query_request.query,
            generated_code=clean_code,
            result=result_json,
            result_rows=result_rows,
            execution_time_ms=execution_time_ms,
            error_message=error_message,
            datasets_used=[str(d.id) for d in datasets] if datasets else None,
            ai_insights=ai_insights  # üÜï New field!
        )
        
    except Exception as e:
        print(f"‚ùå Query execution failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Query execution failed: {str(e)}"
        )


# ==========================================
# QUERY HISTORY
# ==========================================

@router.get("/history", response_model=QueryHistoryResponse)
def get_query_history(
    limit: int = 50,
    offset: int = 0,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Get user's query history
    
    Returns last N queries with pagination.
    NOTE: Since we disabled history caching, this will return old cached queries only.
    """
    
    # Get total count
    total = db.query(QueryHistory).filter(
        QueryHistory.user_id == current_user.id
    ).count()
    
    # Get queries
    queries = db.query(QueryHistory).filter(
        QueryHistory.user_id == current_user.id
    ).order_by(
        QueryHistory.created_at.desc()
    ).limit(limit).offset(offset).all()
    
    # üîß FIX: Convert UUID to string for Pydantic
    items = []
    for q in queries:
        item_dict = {
            "id": str(q.id),  # Convert UUID to string
            "query_text": q.query_text,
            "result_rows": q.result_rows,
            "execution_time_ms": q.execution_time_ms,
            "success": q.success,
            "created_at": q.created_at.isoformat() if q.created_at else None
        }
        items.append(QueryHistoryItem(**item_dict))
    
    return QueryHistoryResponse(
        total=total,
        items=items
    )


# ==========================================
# GET SINGLE QUERY
# ==========================================

@router.get("/{query_id}", response_model=QueryExecuteResponse)
def get_query_by_id(
    query_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Get specific query by ID
    
    Returns full query details including generated code and results.
    NOTE: Since we disabled history caching, this will only work for old cached queries.
    """
    
    query = db.query(QueryHistory).filter(
        QueryHistory.id == query_id,
        QueryHistory.user_id == current_user.id
    ).first()
    
    if not query:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Query not found"
        )
    
    return QueryExecuteResponse(
        query_id=str(query.id),
        success=query.success,
        query_text=query.query_text,
        generated_code=query.generated_code,
        result=query.result,
        result_rows=query.result_rows,
        execution_time_ms=query.execution_time_ms,
        error_message=query.error_message,
        datasets_used=query.datasets_used
    )


# ==========================================
# SPEECH-TO-TEXT TRANSCRIPTION
# ==========================================

@router.post("/transcribe")
async def transcribe_audio(
    file: UploadFile = File(...)
    # TEMPORARILY DISABLED AUTH FOR TESTING
    # current_user: User = Depends(get_current_user)
):
    """
    Transcribe audio to text using OpenAI Whisper API
    WITH FFMPEG CONVERSION FOR WEBM
    
    Accepts audio files in formats: mp3, mp4, mpeg, mpga, m4a, wav, webm
    Max file size: 25MB (OpenAI limit)
    
    Returns: {"text": "transcribed text"}
    
    Requires: Bearer token
    """
    
    # Validate OpenAI API key
    if not settings.OPENAI_API_KEY:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="OpenAI API key not configured"
        )
    
    # Validate file type
    allowed_extensions = {'.mp3', '.mp4', '.mpeg', '.mpga', '.m4a', '.wav', '.webm'}
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported audio format. Allowed: {', '.join(allowed_extensions)}"
        )
    
    # Check file size (25MB limit from OpenAI)
    MAX_SIZE = 25 * 1024 * 1024  # 25MB
    
    mp3_path = None  # Track converted file for cleanup
    
    try:
        # Save uploaded file to temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp_file:
            content = await file.read()
            
            if len(content) > MAX_SIZE:
                raise HTTPException(
                    status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                    detail="Audio file too large. Maximum size: 25MB"
                )
            
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        # Convert WebM to MP3 if needed (OpenAI doesn't support WebM well)
        if file_ext == '.webm':
            print(f"üîÑ Converting WebM to MP3...")
            mp3_path = tmp_file_path.replace('.webm', '.mp3')
            
            try:
                import ffmpeg
                
                # Convert using ffmpeg
                (
                    ffmpeg
                    .input(tmp_file_path)
                    .output(mp3_path, acodec='libmp3lame', audio_bitrate='128k')
                    .overwrite_output()
                    .run(capture_stdout=True, capture_stderr=True)
                )
                
                print(f"‚úÖ Converted to MP3: {mp3_path}")
                
                # Use converted file for transcription
                transcription_file = mp3_path
                
            except Exception as conv_err:
                print(f"‚ö†Ô∏è FFmpeg conversion failed: {conv_err}")
                print(f"Trying with original WebM file...")
                transcription_file = tmp_file_path
        else:
            transcription_file = tmp_file_path
        
        # Transcribe using OpenAI Whisper
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=settings.OPENAI_API_KEY)
            
            with open(transcription_file, 'rb') as audio_file:
                transcript = client.audio.transcriptions.create(
                    model=settings.OPENAI_WHISPER_MODEL,
                    file=audio_file,
                    language="cs"
                )
            
            transcribed_text = transcript.text
            
            print(f"üéôÔ∏è Transcribed: {transcribed_text}")
            
            return {
                "text": transcribed_text,
                "success": True
            }
            
        except Exception as e:
            print(f"‚ùå OpenAI transcription error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Transcription failed: {str(e)}"
            )
        
        finally:
            # Clean up temporary files
            try:
                os.unlink(tmp_file_path)
                if mp3_path and os.path.exists(mp3_path):
                    os.unlink(mp3_path)
            except:
                pass
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå Transcription request failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(e)
        )


# ==========================================
# AI ANALYST CHAT ENDPOINT
# ==========================================

@router.post("/chat", response_model=ChatResponse)
async def chat_with_analyst(
    chat_request: ChatRequest,
    current_user: User = Depends(get_current_user)
):
    """
    Interactive chat with AI Analyst about query insights
    
    Accepts:
    - message: User's question
    - context: Insights context (summary, findings, recommendations, etc.)
    - conversation_history: Previous messages in conversation
    
    Returns:
    - response: AI analyst's answer in Czech
    """
    
    try:
        # Build system prompt with context
        system_prompt = f"""Jsi zku≈°en√Ω business analytik, kter√Ω pom√°h√° u≈æivateli porozumƒõt v√Ωsledk≈Øm jejich datov√© anal√Ωzy.

**P≈ÆVODN√ç DOTAZ:**
{chat_request.context.query_text}

**V√ùSLEDKY ANAL√ùZY:**

Shrnut√≠:
{chat_request.context.summary}

Kl√≠ƒçov√° zji≈°tƒõn√≠:
{chr(10).join(f"- {finding}" for finding in chat_request.context.key_findings)}

Doporuƒçen√≠:
{chr(10).join(f"- {rec.get('title', '')}: {rec.get('description', '')}" for rec in chat_request.context.recommendations)}

Rizika:
{chr(10).join(f"- {risk}" for risk in chat_request.context.risks)}

P≈ô√≠le≈æitosti:
{chr(10).join(f"- {opp}" for opp in chat_request.context.opportunities)}

**TV≈ÆJ √öKOL:**
U≈æivatel se pt√° na up≈ôes≈àuj√≠c√≠ ot√°zky k t√©to anal√Ωze. Odpov√≠dej:
- V ƒçe≈°tinƒõ
- Struƒçnƒõ a jasnƒõ
- S konkr√©tn√≠mi ƒç√≠sly a fakty z anal√Ωzy
- Business-focused (zamƒõ≈ô se na akce a dopady)
- Pokud informace v kontextu nen√≠, up≈ô√≠mnƒõ to ≈ôekni

Buƒè profesion√°ln√≠, ale p≈ô√°telsk√Ω. C√≠lem je pomoci u≈æivateli l√©pe pochopit data a udƒõlat spr√°vn√° rozhodnut√≠.
"""

        # Build messages array
        messages = []
        
        # Add conversation history
        for msg in chat_request.conversation_history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        # Add current message
        messages.append({
            "role": "user",
            "content": chat_request.message
        })
        
        # Call Claude API
        client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1500,
            system=system_prompt,
            messages=messages
        )
        
        assistant_response = response.content[0].text
        
        print(f"üí¨ AI Analyst Chat - User: {chat_request.message[:50]}... ‚Üí AI: {assistant_response[:50]}...")
        
        return ChatResponse(
            response=assistant_response,
            success=True
        )
    
    except Exception as e:
        print(f"‚ùå AI Analyst Chat error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get AI response: {str(e)}"
        )


# ==========================================
# AI ANALYST ANALYZE ENDPOINT
# ==========================================

@router.post("/analyze", response_model=AnalysisResponse)
async def analyze_query_results(
    request: AnalyzeRequest,
    current_user: User = Depends(get_current_user)
):
    """
    AI Analytik - Profesion√°ln√≠ finanƒçn√≠ anal√Ωza v√Ωsledk≈Ø dotazu
    
    Analyzuje SKUTEƒåN√Å data z tabulky (ne jen k√≥d) a poskytuje:
    - Finanƒçn√≠ overview s konkr√©tn√≠mi ƒç√≠sly
    - Kl√≠ƒçov√© poznatky
    - Business doporuƒçen√≠
    
    ZAK√ÅZ√ÅNO:
    - Generick√° sdƒõlen√≠
    - Vym√Ω≈°len√≠ dat
    - Anal√Ωza jin√Ωch dat ne≈æ poskytnut√Ωch
    """
    
    try:
        print(f"üìä AI Analyst Analyze - User: {current_user.email}, Query: {request.query[:50]}...")
        print(f"üìä Data sample: {len(request.data_sample)} rows, Total: {request.total_rows} rows")
        
        # Form√°tuj data do ƒçiteln√© tabulky
        if not request.data_sample:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No data provided for analysis"
            )
        
        # Vytvo≈ô textovou reprezentaci tabulky
        table_text = "DATA K ANAL√ùZE:\n\n"
        
        # Header
        columns = request.columns
        table_text += " | ".join(columns) + "\n"
        table_text += "-" * 80 + "\n"
        
        # Rows (first 10)
        for row in request.data_sample[:10]:
            row_values = [str(row.get(col, "N/A")) for col in columns]
            table_text += " | ".join(row_values) + "\n"
        
        if request.total_rows > 10:
            table_text += f"\n... (celkem {request.total_rows} ≈ô√°dk≈Ø)\n"
        
        # Detekce typu dat (n√°klady vs tr≈æby)
        sample_values = []
        for row in request.data_sample:
            for col in columns:
                val = row.get(col)
                if isinstance(val, (int, float)) and val != 0:
                    sample_values.append(val)
                    if len(sample_values) >= 5:
                        break
            if len(sample_values) >= 5:
                break
        
        is_expenses = any(v < 0 for v in sample_values)
        data_type = "N√ÅKLADY (z√°porn√© hodnoty)" if is_expenses else "TR≈ΩBY nebo JIN√Å DATA"
        
        # Vytvo≈ô prompt pro Claude
        prompt = f"""Jsi senior finanƒçn√≠ analytik pro Alza.cz s expert√≠zou v controllingu a business intelligence.

DOTAZ U≈ΩIVATELE:
"{request.query}"

{table_text}

TYP DAT: {data_type}

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è KRITICK√Å PRAVIDLA:

1. Analyzuj POUZE tato konkr√©tn√≠ data - ≈Ω√ÅDN√Å vymy≈°len√° ƒç√≠sla!
2. Pou≈æ√≠vej P≈òESN√â hodnoty z tabulky v√Ω≈°e
3. Pokud jsou hodnoty Z√ÅPORN√â, jedn√° se o N√ÅKLADY (ne tr≈æby!)
4. ≈Ω√ÅDN√Å generick√° sdƒõlen√≠ jako "data vykazuj√≠ sez√≥nnost" bez konkr√©tn√≠ch ƒç√≠sel
5. V≈ædy uveƒè KONKR√âTN√ç ƒç√°stky/procenta z tabulky
6. Zamƒõ≈ô se na FINANƒåN√ç a BUSINESS implikace

FORM√ÅT ODPOVƒöDI:

ANAL√ùZA: (2-3 vƒõty shrnut√≠ s konkr√©tn√≠mi ƒç√≠sly z tabulky)

KL√çƒåOV√â POZNATKY:
- [Poznatek 1 s konkr√©tn√≠m ƒç√≠slem]
- [Poznatek 2 s konkr√©tn√≠m ƒç√≠slem]
- [Poznatek 3 s konkr√©tn√≠m ƒç√≠slem]
- [Poznatek 4 - pokud relevantn√≠]
- [Poznatek 5 - pokud relevantn√≠]

DOPORUƒåEN√ç:
- [Doporuƒçen√≠ 1 pro management]
- [Doporuƒçen√≠ 2 pro management]
- [Doporuƒçen√≠ 3 - pokud relevantn√≠]

P≈ò√çKLAD DOBR√â ANAL√ùZY:
"Spot≈ôeba materi√°lu ƒçin√≠ 5,577,762 Kƒç, co≈æ p≈ôedstavuje 67% celkov√Ωch n√°klad≈Ø. Energie s 2,765,010 Kƒç tvo≈ô√≠ zb√Ωvaj√≠c√≠ch 33%."

P≈ò√çKLAD ≈†PATN√â ANAL√ùZY (NEPOU≈Ω√çVEJ!):
"Data vykazuj√≠ znaƒçnou variabilitu. Doporuƒçujeme monitorovat trendy."

Pi≈° v ƒçe≈°tinƒõ, profesion√°lnƒõ, s konkr√©tn√≠mi ƒç√≠sly!"""

        # Zavolej Claude API
        client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        
        message = client.messages.create(
            model=settings.ANTHROPIC_MODEL,
            max_tokens=1500,
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )
        
        response_text = message.content[0].text
        
        # Parsuj odpovƒõƒè
        lines = response_text.strip().split('\n')
        
        analysis = ""
        insights = []
        recommendations = []
        
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.upper().startswith('ANAL√ùZA:'):
                current_section = 'analysis'
                analysis = line.replace('ANAL√ùZA:', '').strip()
            elif line.upper().startswith('KL√çƒåOV√â POZNATKY:'):
                current_section = 'insights'
            elif line.upper().startswith('DOPORUƒåEN√ç:'):
                current_section = 'recommendations'
            elif line.startswith('-') or line.startswith('‚Ä¢'):
                content = line.lstrip('-‚Ä¢').strip()
                if current_section == 'insights':
                    insights.append(content)
                elif current_section == 'recommendations':
                    recommendations.append(content)
            elif current_section == 'analysis' and line:
                analysis += " " + line
        
        # Fallback pokud parsov√°n√≠ selhalo
        if not analysis:
            analysis = response_text[:300]
        if not insights:
            insights = ["Anal√Ωza dokonƒçena - viz celkov√© shrnut√≠"]
        if not recommendations:
            recommendations = ["Doporuƒçen√≠ nejsou k dispozici"]
        
        print(f"‚úÖ AI Analyst Analyze - Analysis generated: {len(analysis)} chars, {len(insights)} insights")
        
        return AnalysisResponse(
            analysis=analysis.strip(),
            insights=insights[:5],  # Max 5 insights
            recommendations=recommendations[:3]  # Max 3 recommendations
        )
    
    except anthropic.APIError as e:
        print(f"‚ùå Claude API error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"AI analysis failed: {str(e)}"
        )
    except Exception as e:
        print(f"‚ùå Analysis error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to analyze results: {str(e)}"
        )


# ==============================================================================
# üÜï ADD DIMENSION - P≈ôidat sloupec k existuj√≠c√≠mu query
# ==============================================================================

class AddDimensionRequest(BaseModel):
    """Request to add dimension to existing query"""
    query_id: str
    dimension: str


class AddDimensionResponse(BaseModel):
    """Response with expanded results"""
    success: bool
    result: Optional[List[dict]] = None
    result_rows: Optional[int] = None
    added_dimension: str
    available_dimensions: Optional[List[str]] = None  # üÜï ALL columns from dataset
    error: Optional[str] = None


@router.post("/add-dimension", response_model=AddDimensionResponse)
async def add_dimension_to_query(
    request: AddDimensionRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Add a dimension (column) to existing query results
    
    Process:
    1. Load original query from history
    2. Load original datasets
    3. Modify code to add dimension to groupby
    4. Re-execute modified code
    5. Return expanded results
    """
    
    try:
        print(f"üìä Adding dimension '{request.dimension}' to query {request.query_id}")
        
        # Load original query
        query_history = db.query(QueryHistory).filter(
            QueryHistory.id == request.query_id,
            QueryHistory.user_id == current_user.id
        ).first()
        
        if not query_history:
            return AddDimensionResponse(
                success=False,
                added_dimension=request.dimension,
                error="Query not found"
            )
        
        if not query_history.success:
            return AddDimensionResponse(
                success=False,
                added_dimension=request.dimension,
                error="Cannot add dimension to failed query"
            )
        
        original_code = query_history.generated_code
        datasets_used = query_history.datasets_used if isinstance(query_history.datasets_used, list) else (json.loads(query_history.datasets_used) if query_history.datasets_used else [])
        
        print(f"‚úÖ Loaded original query: {query_history.query_text}")
        
        # Load datasets
        dataframes = {}
        
        for dataset_id in datasets_used:
            dataset = db.query(Dataset).filter(
                Dataset.id == dataset_id,
                Dataset.tenant_id == current_user.tenant_id
            ).first()
            
            if dataset:
                try:
                    if dataset.filename.endswith('.csv'):
                        df = pd.read_csv(
                            dataset.file_path,
                            encoding='utf-8',
                            sep=';',
                            on_bad_lines='skip'
                        )
                    else:
                        df = pd.read_excel(dataset.file_path)
                    
                    df_name = dataset.original_filename.replace('.csv', '').replace('.xlsx', '').replace('.xls', '')
                    dataframes[df_name] = df
                    print(f"‚úÖ Loaded: {df_name}")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load dataset: {e}")
        
        # üÜï Get ALL available dimensions from datasets
        all_dimensions = []
        for df_name, df in dataframes.items():
            all_dimensions.extend(df.columns.tolist())
        all_dimensions = list(set(all_dimensions))
        print(f"üîç Available dimensions ({len(all_dimensions)}): {all_dimensions[:10]}...")

        print(f"üîç Dataframes: {list(dataframes.keys())}")
        print(f"üîç Datasets used: {datasets_used}")
        if not dataframes:
            return AddDimensionResponse(
                success=False,
                added_dimension=request.dimension,
                available_dimensions=all_dimensions,
                error="No datasets available"
            )
        
        # Validate dimension exists
        dimension_found = False
        for df_name, df in dataframes.items():
            if request.dimension in df.columns:
                dimension_found = True
                break
        
        if not dimension_found:
            return AddDimensionResponse(
                success=False,
                added_dimension=request.dimension,
                available_dimensions=all_dimensions,
                error=f"Dimension '{request.dimension}' not found in datasets"
            )
        
        # Modify code to add dimension
        import re
        modified = original_code
        
        # Pattern 1: groupby('col') -> groupby(['col', 'dim'])
        pattern1 = r"\.groupby\('([^']+)'\)"
        def replace1(m):
            return f".groupby(['{m.group(1)}', '{request.dimension}'])"
        modified = re.sub(pattern1, replace1, modified)
        
        # Pattern 2: groupby(['A']) -> groupby(['A', 'dim'])
        pattern2 = r"\.groupby\(\[([^\]]+)\]\)"
        def replace2(m):
            return f".groupby([{m.group(1)}, '{request.dimension}'])"
        modified = re.sub(pattern2, replace2, modified)
        
        # Add reset_index() if not present
        if 'reset_index()' not in modified and 'groupby' in modified:
            lines = modified.split('\n')
            new_lines = []
            for line in lines:
                if 'result =' in line and 'groupby' in line:
                    if not line.strip().endswith('.reset_index()'):
                        line = line.rstrip() + '.reset_index()'
                new_lines.append(line)
            modified = '\n'.join(new_lines)
        
        print(f"üîß Modified code:\n{modified[:500]}...")
        
        # Execute modified code
        exec_globals = {'pd': pd, 'DataFrame': pd.DataFrame, **dataframes}
        exec_locals = {}
        exec(modified, exec_globals, exec_locals)
        
        result_df = exec_locals.get('result')
        
        if result_df is None or not isinstance(result_df, pd.DataFrame):
            return AddDimensionResponse(
                success=False,
                added_dimension=request.dimension,
                error="Failed to generate result"
            )
        
        print(f"‚úÖ Result: {len(result_df)} rows, {len(result_df.columns)} cols")
        
        result_json = result_df.to_dict('records')
        
        # üÜï Get ALL available dimensions from original dataset
        all_dimensions = []
        for df_name, df in dataframes.items():
            all_dimensions.extend(df.columns.tolist())
        # Remove duplicates and already present columns
        all_dimensions = list(set(all_dimensions))
        
        print(f"üîç Available dimensions: {all_dimensions}")
        print(f"üîç Available dimensions: {all_dimensions}")
        return AddDimensionResponse(
            success=True,
            result=result_json,
            result_rows=len(result_df),
            added_dimension=request.dimension,
            available_dimensions=all_dimensions  # üÜï ALL columns from dataset
        )
        
    except Exception as e:
        print(f"‚ùå Error adding dimension: {e}")
        import traceback
        traceback.print_exc()
        
        return AddDimensionResponse(
            success=False,
            added_dimension=request.dimension,
            error=str(e)
        )


# ==========================================
# üí¨ AI ANALYST CHAT ENDPOINT
# ==========================================

class ChatRequest(BaseModel):
    """Request model for AI analyst chat"""
    message: str
    context: Dict[str, Any]
    conversation_history: List[Dict[str, str]] = []


class ChatResponse(BaseModel):
    """Response model for AI analyst chat"""
    response: str


